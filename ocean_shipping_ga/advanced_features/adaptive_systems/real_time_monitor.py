#!/usr/bin/env python3
"""
ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
ì‹œìŠ¤í…œ ì„±ëŠ¥, í™˜ê²½ ë³€í™”, ë°ì´í„° í’ˆì§ˆì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
"""

import sys
import os
import time
import threading
import queue
import numpy as np
import pandas as pd
from typing import Dict, List, Callable, Optional, Any
from datetime import datetime, timedelta
import json
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from models.parameters import GAParameters
from config import get_constant


class RealTimeMonitor:
    """ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, ga_parameters: GAParameters, 
                 monitoring_interval: float = 10.0,
                 alert_thresholds: Optional[Dict] = None):
        """
        Parameters:
        -----------
        ga_parameters : GAParameters
            ëª¨ë‹ˆí„°ë§í•  GA íŒŒë¼ë¯¸í„°
        monitoring_interval : float
            ëª¨ë‹ˆí„°ë§ ê°„ê²© (ì´ˆ)
        alert_thresholds : Dict
            ì•Œë¦¼ ì„ê³„ê°’ ì„¤ì •
        """
        self.ga_params = ga_parameters
        self.monitoring_interval = monitoring_interval
        
        # ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì • (ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ)
        self.alert_thresholds = alert_thresholds or {
            'performance_degradation': get_constant('monitoring.alert_thresholds.performance_degradation', 0.2),  # 20% ì„±ëŠ¥ ì €í•˜
            'data_anomaly_score': get_constant('monitoring.alert_thresholds.data_anomaly_score', 0.8),           # 80% ì´ìƒ ì´ìƒì¹˜
            'system_load': get_constant('monitoring.alert_thresholds.system_load', 0.9),                        # 90% ì‹œìŠ¤í…œ ë¶€í•˜
            'memory_usage': get_constant('monitoring.alert_thresholds.memory_usage', 0.85),                     # 85% ë©”ëª¨ë¦¬ ì‚¬ìš©
            'response_time': get_constant('monitoring.alert_thresholds.response_time', 30.0)                    # 30ì´ˆ ì‘ë‹µì‹œê°„
        }
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.is_monitoring = False
        self.monitor_thread = None
        self.data_queue = queue.Queue()
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        self.metrics_history = []
        self.alerts_history = []
        self.performance_baseline = None
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        self.alert_callbacks = []
        self.metric_callbacks = []
        
        # ë¡œê¹… ì„¤ì •
        self.logger = self._setup_logging()
        
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        logger = logging.getLogger('RealTimeMonitor')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def start_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            self.logger.warning("Monitoring is already running")
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        self.logger.info(f"ğŸ” Real-time monitoring started (interval: {self.monitoring_interval}s)")
    
    def stop_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if not self.is_monitoring:
            self.logger.warning("Monitoring is not running")
            return
        
        self.is_monitoring = False
        
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5.0)
        
        self.logger.info("ğŸ›‘ Real-time monitoring stopped")
    
    def _monitoring_loop(self):
        """ë©”ì¸ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        self.logger.info("ğŸ“¡ Monitoring loop started")
        
        while self.is_monitoring:
            try:
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics = self._collect_metrics()
                
                # ì´ìƒ ê°ì§€
                anomalies = self._detect_anomalies(metrics)
                
                # ì•Œë¦¼ ì²˜ë¦¬
                if anomalies:
                    self._handle_alerts(anomalies, metrics)
                
                # ë©”íŠ¸ë¦­ ì €ì¥
                self._store_metrics(metrics, anomalies)
                
                # ì½œë°± ì‹¤í–‰
                self._execute_callbacks(metrics, anomalies)
                
                # ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ ëŒ€ê¸°
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                self.logger.error(f"Monitoring error: {e}")
                time.sleep(self.monitoring_interval)
        
        self.logger.info("ğŸ“¡ Monitoring loop ended")
    
    def _collect_metrics(self) -> Dict:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        metrics = {
            'timestamp': datetime.now(),
            'system_metrics': self._collect_system_metrics(),
            'data_metrics': self._collect_data_metrics(),
            'performance_metrics': self._collect_performance_metrics(),
            'ga_metrics': self._collect_ga_metrics()
        }
        
        return metrics
    
    def _collect_system_metrics(self) -> Dict:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            import psutil
            
            return {
                'cpu_percent': psutil.cpu_percent(interval=1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('/').percent,
                'network_io': psutil.net_io_counters()._asdict() if psutil.net_io_counters() else {},
                'process_count': len(psutil.pids()),
                'boot_time': psutil.boot_time()
            }
        except ImportError:
            # psutilì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”íŠ¸ë¦­ë§Œ
            return {
                'cpu_percent': 0,
                'memory_percent': 0,
                'disk_usage': 0,
                'network_io': {},
                'process_count': 0,
                'boot_time': 0
            }
    
    def _collect_data_metrics(self) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        schedule_data = self.ga_params.schedule_data
        
        return {
            'total_schedules': len(schedule_data),
            'data_completeness': (1 - schedule_data.isnull().sum().sum() / schedule_data.size),
            'data_freshness': (datetime.now() - schedule_data['ETD'].max()).days,
            'demand_variance': schedule_data['ì£¼ë¬¸ëŸ‰(KG)'].var() if 'ì£¼ë¬¸ëŸ‰(KG)' in schedule_data else 0,
            'schedule_distribution': self._calculate_schedule_distribution(),
            'anomalous_values': self._detect_data_anomalies()
        }
    
    def _collect_performance_metrics(self) -> Dict:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        # ë”ë¯¸ ì„±ëŠ¥ ë°ì´í„° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ìµœê·¼ ìµœì í™” ê²°ê³¼ ì‚¬ìš©)
        return {
            'avg_response_time': np.random.normal(15, 3),  # í‰ê·  15ì´ˆ, í‘œì¤€í¸ì°¨ 3ì´ˆ
            'optimization_success_rate': np.random.uniform(0.85, 0.98),
            'convergence_speed': np.random.normal(25, 5),  # í‰ê·  25ì„¸ëŒ€
            'solution_quality': np.random.uniform(-5000, -3000),  # fitness ê°’
            'memory_efficiency': np.random.uniform(0.7, 0.9)
        }
    
    def _collect_ga_metrics(self) -> Dict:
        """GA íŠ¹í™” ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        return {
            'population_diversity': self._calculate_population_diversity(),
            'constraint_violations': self._count_constraint_violations(),
            'parameter_stability': self._measure_parameter_stability(),
            'adaptive_adjustments': self._count_recent_adjustments()
        }
    
    def _calculate_schedule_distribution(self) -> Dict:
        """ìŠ¤ì¼€ì¤„ ë¶„í¬ ê³„ì‚°"""
        schedule_data = self.ga_params.schedule_data
        
        # ì‹œê°„ë³„ ë¶„í¬
        schedule_data['hour'] = pd.to_datetime(schedule_data['ETD']).dt.hour
        hourly_dist = schedule_data['hour'].value_counts()
        
        return {
            'hourly_std': hourly_dist.std(),
            'peak_hour': hourly_dist.idxmax(),
            'min_hour': hourly_dist.idxmin(),
            'distribution_entropy': self._calculate_entropy(hourly_dist.values)
        }
    
    def _detect_data_anomalies(self) -> int:
        """ë°ì´í„° ì´ìƒì¹˜ ê°ì§€"""
        schedule_data = self.ga_params.schedule_data
        anomalies = 0
        
        # ì£¼ë¬¸ëŸ‰ ì´ìƒì¹˜
        if 'ì£¼ë¬¸ëŸ‰(KG)' in schedule_data:
            demand_data = schedule_data['ì£¼ë¬¸ëŸ‰(KG)'].dropna()
            if len(demand_data) > 0:
                Q1 = demand_data.quantile(0.25)
                Q3 = demand_data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                anomalies += ((demand_data < lower_bound) | (demand_data > upper_bound)).sum()
        
        return anomalies
    
    def _calculate_population_diversity(self) -> float:
        """ì¸êµ¬ ë‹¤ì–‘ì„± ê³„ì‚° (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œë¡œëŠ” í˜„ì¬ GA ì¸êµ¬ì˜ ë‹¤ì–‘ì„±ì„ ê³„ì‚°
        return np.random.uniform(0.3, 0.8)
    
    def _count_constraint_violations(self) -> int:
        """ì œì•½ ìœ„ë°˜ ê°œìˆ˜ (ì‹œë®¬ë ˆì´ì…˜)"""
        return np.random.poisson(5)  # í‰ê·  5ê°œ ìœ„ë°˜
    
    def _measure_parameter_stability(self) -> float:
        """íŒŒë¼ë¯¸í„° ì•ˆì •ì„± ì¸¡ì •"""
        # ìµœê·¼ ë©”íŠ¸ë¦­ ì´ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
        if len(self.metrics_history) < 3:
            return 1.0
        
        recent_metrics = self.metrics_history[-3:]
        stability_scores = []
        
        for metric_name in ['performance_metrics']:
            values = [m[metric_name].get('solution_quality', 0) for m in recent_metrics]
            if len(values) > 1:
                stability = 1 / (1 + np.std(values))
                stability_scores.append(stability)
        
        return np.mean(stability_scores) if stability_scores else 1.0
    
    def _count_recent_adjustments(self) -> int:
        """ìµœê·¼ ì ì‘ ì¡°ì • íšŸìˆ˜"""
        # ì‹¤ì œë¡œëŠ” DynamicUpdaterì˜ ì´ë ¥ í™•ì¸
        return np.random.poisson(2)
    
    def _calculate_entropy(self, values: np.ndarray) -> float:
        """ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        if len(values) == 0:
            return 0
        
        probs = values / values.sum()
        probs = probs[probs > 0]  # 0 í™•ë¥  ì œê±°
        
        return -np.sum(probs * np.log2(probs))
    
    def _detect_anomalies(self, metrics: Dict) -> List[Dict]:
        """ì´ìƒ ìƒí™© ê°ì§€"""
        anomalies = []
        
        # ì„±ëŠ¥ ì €í•˜ ê°ì§€
        current_quality = metrics['performance_metrics']['solution_quality']
        if self.performance_baseline is not None:
            degradation = (self.performance_baseline - current_quality) / abs(self.performance_baseline)
            if degradation > self.alert_thresholds['performance_degradation']:
                anomalies.append({
                    'type': 'performance_degradation',
                    'severity': 'high',
                    'value': degradation,
                    'threshold': self.alert_thresholds['performance_degradation'],
                    'message': f"Performance degraded by {degradation:.2%}"
                })
        
        # ì‹œìŠ¤í…œ ë¶€í•˜ ê°ì§€
        cpu_usage = metrics['system_metrics']['cpu_percent'] / 100
        if cpu_usage > self.alert_thresholds['system_load']:
            anomalies.append({
                'type': 'high_system_load',
                'severity': 'medium',
                'value': cpu_usage,
                'threshold': self.alert_thresholds['system_load'],
                'message': f"High CPU usage: {cpu_usage:.1%}"
            })
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì§€
        memory_usage = metrics['system_metrics']['memory_percent'] / 100
        if memory_usage > self.alert_thresholds['memory_usage']:
            anomalies.append({
                'type': 'high_memory_usage',
                'severity': 'medium',
                'value': memory_usage,
                'threshold': self.alert_thresholds['memory_usage'],
                'message': f"High memory usage: {memory_usage:.1%}"
            })
        
        # ì‘ë‹µì‹œê°„ ê°ì§€
        response_time = metrics['performance_metrics']['avg_response_time']
        if response_time > self.alert_thresholds['response_time']:
            anomalies.append({
                'type': 'slow_response',
                'severity': 'low',
                'value': response_time,
                'threshold': self.alert_thresholds['response_time'],
                'message': f"Slow response time: {response_time:.1f}s"
            })
        
        # ë°ì´í„° ì´ìƒì¹˜ ê°ì§€
        anomaly_count = metrics['data_metrics']['anomalous_values']
        total_data = metrics['data_metrics']['total_schedules']
        anomaly_ratio = anomaly_count / total_data if total_data > 0 else 0
        
        if anomaly_ratio > self.alert_thresholds['data_anomaly_score']:
            anomalies.append({
                'type': 'data_anomaly',
                'severity': 'high',
                'value': anomaly_ratio,
                'threshold': self.alert_thresholds['data_anomaly_score'],
                'message': f"High data anomaly rate: {anomaly_ratio:.1%}"
            })
        
        return anomalies
    
    def _handle_alerts(self, anomalies: List[Dict], metrics: Dict):
        """ì•Œë¦¼ ì²˜ë¦¬"""
        for anomaly in anomalies:
            alert = {
                'timestamp': datetime.now(),
                'anomaly': anomaly,
                'metrics_snapshot': metrics,
                'alert_id': f"alert_{len(self.alerts_history)}"
            }
            
            self.alerts_history.append(alert)
            
            # ë¡œê·¸ ì¶œë ¥
            severity_emoji = {'low': 'ğŸŸ¡', 'medium': 'ğŸŸ ', 'high': 'ğŸ”´'}
            emoji = severity_emoji.get(anomaly['severity'], 'âšª')
            
            self.logger.warning(f"{emoji} ALERT [{anomaly['type']}]: {anomaly['message']}")
    
    def _store_metrics(self, metrics: Dict, anomalies: List[Dict]):
        """ë©”íŠ¸ë¦­ ì €ì¥"""
        metrics_entry = {
            'timestamp': metrics['timestamp'],
            'metrics': metrics,
            'anomalies': anomalies,
            'anomaly_count': len(anomalies)
        }
        
        self.metrics_history.append(metrics_entry)
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 1000ê°œë§Œ ë³´ê´€)
        if len(self.metrics_history) > 1000:
            self.metrics_history = self.metrics_history[-1000:]
    
    def _execute_callbacks(self, metrics: Dict, anomalies: List[Dict]):
        """ë“±ë¡ëœ ì½œë°± í•¨ìˆ˜ ì‹¤í–‰"""
        # ë©”íŠ¸ë¦­ ì½œë°±
        for callback in self.metric_callbacks:
            try:
                callback(metrics)
            except Exception as e:
                self.logger.error(f"Metric callback error: {e}")
        
        # ì•Œë¦¼ ì½œë°±
        if anomalies:
            for callback in self.alert_callbacks:
                try:
                    callback(anomalies, metrics)
                except Exception as e:
                    self.logger.error(f"Alert callback error: {e}")
    
    def register_alert_callback(self, callback: Callable):
        """ì•Œë¦¼ ì½œë°± ë“±ë¡"""
        self.alert_callbacks.append(callback)
        self.logger.info("Alert callback registered")
    
    def register_metric_callback(self, callback: Callable):
        """ë©”íŠ¸ë¦­ ì½œë°± ë“±ë¡"""
        self.metric_callbacks.append(callback)
        self.logger.info("Metric callback registered")
    
    def set_performance_baseline(self, baseline_value: float):
        """ì„±ëŠ¥ ê¸°ì¤€ì„  ì„¤ì •"""
        self.performance_baseline = baseline_value
        self.logger.info(f"Performance baseline set: {baseline_value:.2f}")
    
    def get_current_status(self) -> Dict:
        """í˜„ì¬ ëª¨ë‹ˆí„°ë§ ìƒíƒœ ë°˜í™˜"""
        if not self.metrics_history:
            return {'status': 'no_data'}
        
        latest_metrics = self.metrics_history[-1]
        recent_alerts = [a for a in self.alerts_history if 
                        (datetime.now() - a['timestamp']).seconds < 3600]  # ìµœê·¼ 1ì‹œê°„
        
        return {
            'monitoring_active': self.is_monitoring,
            'latest_metrics_time': latest_metrics['timestamp'],
            'recent_alerts_count': len(recent_alerts),
            'total_metrics_collected': len(self.metrics_history),
            'system_health': self._assess_system_health(),
            'performance_baseline': self.performance_baseline
        }
    
    def _assess_system_health(self) -> str:
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ í‰ê°€"""
        if not self.metrics_history:
            return 'unknown'
        
        recent_alerts = [a for a in self.alerts_history if 
                        (datetime.now() - a['timestamp']).seconds < 3600]
        
        high_severity_alerts = [a for a in recent_alerts if 
                               a['anomaly']['severity'] == 'high']
        
        if len(high_severity_alerts) > 0:
            return 'critical'
        elif len(recent_alerts) > 5:
            return 'warning'
        else:
            return 'healthy'
    
    def export_metrics(self, filepath: str, hours_back: int = 24):
        """ë©”íŠ¸ë¦­ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        filtered_metrics = [
            m for m in self.metrics_history 
            if m['timestamp'] >= cutoff_time
        ]
        
        try:
            # JSONìœ¼ë¡œ ì €ì¥ (datetime ì§ë ¬í™” ì²˜ë¦¬)
            def datetime_converter(obj):
                if isinstance(obj, datetime):
                    return obj.isoformat()
                raise TypeError
            
            with open(filepath, 'w') as f:
                json.dump(filtered_metrics, f, default=datetime_converter, indent=2)
            
            self.logger.info(f"ğŸ“Š Exported {len(filtered_metrics)} metrics to {filepath}")
            
        except Exception as e:
            self.logger.error(f"Failed to export metrics: {e}")